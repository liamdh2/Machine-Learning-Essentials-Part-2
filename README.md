# Machine Learning Essentials Part II

## Overview
This repository continues my hands‑on machine learning journey, covering a broad spectrum of algorithms and techniques— from regression and clustering to dimensionality reduction, generative AI, and NLP. Each folder contains Jupyter notebooks that explore these topics through practical examples and real datasets.

---

### 📁 `data/`
Provides the raw datasets used throughout Part II:
- **`enrollment_forecast.csv`**: Historical enrollment figures for time‑series forecasting practice.  
- **`groceries.csv`**: Market basket transaction data for association rule mining.  
- **`iris.data.csv`**: Classic Iris flower measurements for classification examples.  
- **`mtcars.csv`**: Motor Trend Car Road Tests dataset for regression tutorials.  
- **`titanic-training-data.csv`**: Titanic passenger records for survival prediction exercises.  

---

### 📁 `Regression Models/`
Explores foundational regression techniques:
- **`Linear Regression.ipynb`**  
  Implements simple linear regression and evaluates model fit with RMSE and R².  
- **`Multi Linear Regression.ipynb`**  
  Extends to multiple predictors, demonstrating feature selection and multicollinearity checks.  
- **`Logistic Regression Project.ipynb`**  
  Applies logistic regression to binary classification tasks (e.g., survival prediction).  

---

### 📁 `Clustering Models/`
Demonstrates unsupervised grouping and outlier detection:
- **`K-means Cluster Analysis.ipynb`**  
  Partitions data into k clusters, visualizes centroids, and evaluates inertia.  
- **`Hierarchical Cluster Analysis.ipynb`**  
  Builds dendrograms to show nested cluster relationships using agglomerative methods.  
- **`DBSCAN for Outlier Detection.ipynb`**  
  Uses density‑based clustering to identify core points and detect anomalies.  

---

### 📁 `Common Machine Learning Models/`
Covers a variety of core supervised learning algorithms:
- **`Instance based learning with KNN.ipynb`**  
  Examines k‑nearest neighbors for classification and regression, tuning k.  
- **`Naive Bayes with Bayesian Statistics.ipynb`**  
  Implements Gaussian and multinomial Naive Bayes classifiers on text and numeric data.  
- **`Neural Networks with perceptrons.ipynb`**  
  Builds a basic single‑layer perceptron network to learn linear decision boundaries.  
- **`Ensemble methods with random forest.ipynb`**  
  Trains a random forest ensemble, explores feature importance and out‑of‑bag error.  
- **`Association rules models with Apriori Algo.ipynb`**  
  Discovers frequent itemsets and association rules in transaction data using the Apriori algorithm.  

---

### 📁 `Dimension Reducing Models/`
Introduces techniques to reduce feature space:
- **`PCA_Principal_Component_Analysis.ipynb`**  
  Performs PCA to identify principal components and visualize variance explained.  
- **`Explanatory Factor Analysis.ipynb`**  
  Applies factor analysis to uncover latent factors driving observed variables.  

---

### 📁 `Generative AI/`
Experiments with generative modeling techniques:
- **`Gen AI Model.ipynb`**  
  Explores a simple generative AI pipeline—training and sampling from a model to create synthetic data.  

---

### 📁 `Natural Language Processing/`
Applies basic NLP preprocessing and modeling:
- **`Natural Language Processing.ipynb`**  
  Cleans text data, tokenizes, and demonstrates simple text classification or topic modeling.  
